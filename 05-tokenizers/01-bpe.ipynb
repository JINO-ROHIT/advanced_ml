{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing BPE algorithm\n",
    "\n",
    "1. Undergoes the normalization and pre-tokenization stage.\n",
    "2. The normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents.\n",
    "3. Pre-tokenization split the texts into small entities and the way they do this can be different for different algorithms.\n",
    "4. Get the list of all the unique words in the text corpus.\n",
    "5. Build the vocabulary using the characters used in each words.\n",
    "\n",
    "    ```\n",
    "    If the corpus : [\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"]\n",
    "\n",
    "    vocabulary : [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]\n",
    "    ```\n",
    "\n",
    "6. Now we have a hyperparameter to set, the desired vocab size. To reach this desired size, we keep merging common tokens.\n",
    "7. Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Hi my name is Jino\",\n",
    "    \"I am sike years old\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "    \"Actually my name is Rohit\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self, corpus, vocab_size=50, debug=False):\n",
    "        self.corpus = corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        self.debug = debug\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.splits = {}\n",
    "        self.merges = defaultdict(int)\n",
    "        self.vocab = [\"<|endoftext|>\"]\n",
    "        self.alphabet = []\n",
    "\n",
    "    def _normalization_pre_tokenization(self):\n",
    "        # normalize to lowercase and split on empty spaces\n",
    "        self.corpus = [word.lower() for sent in self.corpus for word in sent.split(' ')]\n",
    "        if self.debug:\n",
    "            print(\"Normalized Corpus:\", self.corpus)\n",
    "            print('\\n')\n",
    "\n",
    "    def _get_frequency(self):\n",
    "        # Compute word frequencies and initial splits\n",
    "        for word in self.corpus:\n",
    "            self.word_freqs[word] += 1\n",
    "        self.splits = {word: list(word) for word in self.word_freqs.keys()}\n",
    "\n",
    "    def _base_vocab(self):\n",
    "        # Generate the base alphabet vocabulary\n",
    "        self.alphabet = sorted({letter for word in self.word_freqs.keys() for letter in word})\n",
    "        self.vocab += self.alphabet.copy()\n",
    "\n",
    "    def compute_pair_freqs(self):\n",
    "        # Compute frequencies of adjacent pairs in current splits\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for word, freq in self.word_freqs.items():\n",
    "            split = self.splits[word]\n",
    "            for i in range(len(split) - 1):\n",
    "                pair = (split[i], split[i + 1])\n",
    "                pair_freqs[pair] += freq\n",
    "        return pair_freqs\n",
    "\n",
    "    def merge_pair(self, a, b):\n",
    "        # Merge the most frequent pair in all words\n",
    "        for word in self.splits.keys():\n",
    "            split = self.splits[word]\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == a and split[i + 1] == b:\n",
    "                    split = split[:i] + [a + b] + split[i + 2:]\n",
    "                else:\n",
    "                    i += 1\n",
    "            self.splits[word] = split\n",
    "\n",
    "    def train(self):\n",
    "        self._normalization_pre_tokenization()\n",
    "        self._get_frequency()\n",
    "        self._base_vocab()\n",
    "\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            pair_freqs = self.compute_pair_freqs()\n",
    "            if not pair_freqs:\n",
    "                break\n",
    "\n",
    "            best_pair = max(pair_freqs, key=pair_freqs.get)\n",
    "            new_token = ''.join(best_pair)\n",
    "            if self.debug:\n",
    "                print(f\"Best Pair to Merge: {best_pair} with Frequency: {pair_freqs[best_pair]}\")\n",
    "            \n",
    "            self.merge_pair(*best_pair)\n",
    "            self.merges[best_pair] = new_token\n",
    "            self.vocab.append(new_token)\n",
    "\n",
    "            if self.debug:\n",
    "                print(\"Updated Vocabulary:\", self.vocab)\n",
    "                print('#################################')\n",
    "                print('\\n')\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Final Vocabulary:\", self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Corpus: ['hi', 'my', 'name', 'is', 'jino', 'i', 'am', 'sike', 'years', 'old', 'this', 'section', 'shows', 'several', 'tokenizer', 'algorithms.', 'hopefully,', 'you', 'will', 'be', 'able', 'to', 'understand', 'how', 'they', 'are', 'trained', 'and', 'generate', 'tokens.', 'actually', 'my', 'name', 'is', 'rohit']\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('e', 'r') with Frequency: 4\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('h', 'i') with Frequency: 3\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('a', 'm') with Frequency: 3\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('k', 'e') with Frequency: 3\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('h', 'o') with Frequency: 3\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('a', 'l') with Frequency: 3\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('t', 'o') with Frequency: 3\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al', 'to']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('n', 'd') with Frequency: 3\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al', 'to', 'nd']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('m', 'y') with Frequency: 2\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al', 'to', 'nd', 'my']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('n', 'am') with Frequency: 2\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al', 'to', 'nd', 'my', 'nam']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('nam', 'e') with Frequency: 2\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al', 'to', 'nd', 'my', 'nam', 'name']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('i', 's') with Frequency: 2\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al', 'to', 'nd', 'my', 'nam', 'name', 'is']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('i', 'n') with Frequency: 2\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al', 'to', 'nd', 'my', 'nam', 'name', 'is', 'in']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('a', 'r') with Frequency: 2\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al', 'to', 'nd', 'my', 'nam', 'name', 'is', 'in', 'ar']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('s', 'e') with Frequency: 2\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al', 'to', 'nd', 'my', 'nam', 'name', 'is', 'in', 'ar', 'se']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('c', 't') with Frequency: 2\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al', 'to', 'nd', 'my', 'nam', 'name', 'is', 'in', 'ar', 'se', 'ct']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('ho', 'w') with Frequency: 2\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al', 'to', 'nd', 'my', 'nam', 'name', 'is', 'in', 'ar', 'se', 'ct', 'how']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('to', 'ke') with Frequency: 2\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al', 'to', 'nd', 'my', 'nam', 'name', 'is', 'in', 'ar', 'se', 'ct', 'how', 'toke']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('toke', 'n') with Frequency: 2\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al', 'to', 'nd', 'my', 'nam', 'name', 'is', 'in', 'ar', 'se', 'ct', 'how', 'toke', 'token']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('t', 'h') with Frequency: 2\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al', 'to', 'nd', 'my', 'nam', 'name', 'is', 'in', 'ar', 'se', 'ct', 'how', 'toke', 'token', 'th']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('s', '.') with Frequency: 2\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al', 'to', 'nd', 'my', 'nam', 'name', 'is', 'in', 'ar', 'se', 'ct', 'how', 'toke', 'token', 'th', 's.']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('l', 'l') with Frequency: 2\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al', 'to', 'nd', 'my', 'nam', 'name', 'is', 'in', 'ar', 'se', 'ct', 'how', 'toke', 'token', 'th', 's.', 'll']\n",
      "#################################\n",
      "\n",
      "\n",
      "Best Pair to Merge: ('a', 'nd') with Frequency: 2\n",
      "Updated Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al', 'to', 'nd', 'my', 'nam', 'name', 'is', 'in', 'ar', 'se', 'ct', 'how', 'toke', 'token', 'th', 's.', 'll', 'and']\n",
      "#################################\n",
      "\n",
      "\n",
      "Final Vocabulary: ['<|endoftext|>', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'er', 'hi', 'am', 'ke', 'ho', 'al', 'to', 'nd', 'my', 'nam', 'name', 'is', 'in', 'ar', 'se', 'ct', 'how', 'toke', 'token', 'th', 's.', 'll', 'and']\n"
     ]
    }
   ],
   "source": [
    "bpe = BPE(corpus, vocab_size = 50, debug = True)\n",
    "bpe.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
