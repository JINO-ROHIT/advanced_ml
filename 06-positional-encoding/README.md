### Positional encoding

Transformers lack the notion of position since every token is processed parallely. We need to add this information by using the positional embedding.

In Book 1, lets look at the sin cosine positional embedding