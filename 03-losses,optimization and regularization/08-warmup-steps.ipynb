{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warmup steps\n",
    "\n",
    "The primacy effect refers to the phenomenon where the model disproportionately weights the patterns it learns from the early batches of data. If the early batches happen to contain a cluster of highly similar or strongly-featured observations, the model may overfit to those patterns early on, leading to suboptimal generalization.\n",
    "\n",
    "This is especially problematic when:\n",
    "\n",
    "- The dataset is highly differentiated (e.g., contains distinct clusters or subgroups).\n",
    "\n",
    "- The data is not perfectly shuffled, leading to batches that are not representative of the overall dataset.\n",
    "\n",
    "- The model is sensitive to initialization (e.g., deep neural networks)\n",
    "\n",
    "\n",
    "**Learning rate warmup** reduces the primacy effect by:\n",
    "\n",
    "1. Starting with a Small Learning Rate: Early updates are small, so the model doesn’t make drastic changes based on the first few batches.\n",
    "\n",
    "2. Gradually Increasing the Learning Rate: As the model sees more data, the learning rate increases, allowing it to make larger updates once it has a better understanding of the overall dataset.\n",
    "\n",
    "This ensures that the model doesn’t overcommit to patterns in the early batches, which may not be representative of the entire dataset.\n",
    "\n",
    "**Warmup Duration**:\n",
    "\n",
    "- Typically, warmup lasts for 1 epoch (i.e., the learning rate reaches its maximum value after the model has seen the entire dataset once).\n",
    "- For highly skewed datasets, you might use a longer warmup period (e.g., 2-3 epochs) to ensure the model has seen enough data to balance the influence of early batches.\n",
    "- For homogeneous datasets, a shorter warmup period (e.g., half an epoch) may suffice.\n",
    "\n",
    "\n",
    "You can have two types of warmup -\n",
    "1. Constant warmup - A fixed warmup value is used for the initial n steps to warmup the network, and then change to your actual learning rate value.\n",
    "2. Linear warmup - In the first few steps, the learning rate is set to be lower than base learning rate and increased gradually to approach it as step number increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100, Learning Rate: 0.000100, Loss: 1.307066\n",
      "Iteration 200, Learning Rate: 0.000200, Loss: 2.096521\n",
      "Iteration 300, Learning Rate: 0.000300, Loss: 1.370643\n",
      "Iteration 400, Learning Rate: 0.000400, Loss: 1.518445\n",
      "Iteration 500, Learning Rate: 0.000500, Loss: 1.430872\n",
      "Iteration 600, Learning Rate: 0.000600, Loss: 1.472371\n",
      "Iteration 700, Learning Rate: 0.000700, Loss: 0.922527\n",
      "Iteration 800, Learning Rate: 0.000800, Loss: 1.153891\n",
      "Iteration 900, Learning Rate: 0.000900, Loss: 1.240970\n",
      "Iteration 1000, Learning Rate: 0.001000, Loss: 0.952746\n",
      "Iteration 1100, Learning Rate: 0.001000, Loss: 0.969623\n",
      "Iteration 1200, Learning Rate: 0.001000, Loss: 1.225448\n",
      "Iteration 1300, Learning Rate: 0.001000, Loss: 0.889674\n",
      "Iteration 1400, Learning Rate: 0.001000, Loss: 0.923594\n",
      "Iteration 1500, Learning Rate: 0.001000, Loss: 0.818584\n",
      "Iteration 1600, Learning Rate: 0.001000, Loss: 1.305761\n",
      "Iteration 1700, Learning Rate: 0.001000, Loss: 0.796680\n",
      "Iteration 1800, Learning Rate: 0.001000, Loss: 1.208030\n",
      "Iteration 1900, Learning Rate: 0.001000, Loss: 0.872426\n",
      "Iteration 2000, Learning Rate: 0.001000, Loss: 0.992274\n",
      "Iteration 2100, Learning Rate: 0.001000, Loss: 0.865567\n",
      "Iteration 2200, Learning Rate: 0.001000, Loss: 0.916192\n",
      "Iteration 2300, Learning Rate: 0.001000, Loss: 1.106205\n",
      "Iteration 2400, Learning Rate: 0.001000, Loss: 1.224398\n",
      "Iteration 2500, Learning Rate: 0.001000, Loss: 0.967572\n",
      "Iteration 2600, Learning Rate: 0.001000, Loss: 1.730540\n",
      "Iteration 2700, Learning Rate: 0.001000, Loss: 1.113512\n",
      "Iteration 2800, Learning Rate: 0.001000, Loss: 0.825805\n",
      "Iteration 2900, Learning Rate: 0.001000, Loss: 0.690020\n",
      "Iteration 3000, Learning Rate: 0.001000, Loss: 0.829847\n",
      "Iteration 3100, Learning Rate: 0.001000, Loss: 1.538435\n",
      "Iteration 3200, Learning Rate: 0.001000, Loss: 1.375922\n",
      "Iteration 3300, Learning Rate: 0.001000, Loss: 0.622410\n",
      "Iteration 3400, Learning Rate: 0.001000, Loss: 1.193682\n",
      "Iteration 3500, Learning Rate: 0.001000, Loss: 0.997975\n",
      "Iteration 3600, Learning Rate: 0.001000, Loss: 0.887730\n",
      "Iteration 3700, Learning Rate: 0.001000, Loss: 1.140303\n",
      "Iteration 3800, Learning Rate: 0.001000, Loss: 0.838140\n",
      "Iteration 3900, Learning Rate: 0.001000, Loss: 0.868355\n",
      "Iteration 4000, Learning Rate: 0.001000, Loss: 1.564393\n",
      "Iteration 4100, Learning Rate: 0.001000, Loss: 0.723226\n",
      "Iteration 4200, Learning Rate: 0.001000, Loss: 1.495438\n",
      "Iteration 4300, Learning Rate: 0.001000, Loss: 0.982865\n",
      "Iteration 4400, Learning Rate: 0.001000, Loss: 1.038152\n",
      "Iteration 4500, Learning Rate: 0.001000, Loss: 0.921953\n",
      "Iteration 4600, Learning Rate: 0.001000, Loss: 1.354623\n",
      "Iteration 4700, Learning Rate: 0.001000, Loss: 0.962691\n",
      "Iteration 4800, Learning Rate: 0.001000, Loss: 0.821562\n",
      "Iteration 4900, Learning Rate: 0.001000, Loss: 1.204638\n",
      "Iteration 5000, Learning Rate: 0.001000, Loss: 0.997792\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = nn.Linear(10, 1)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Warmup parameters\n",
    "warmup_iterations = 1000\n",
    "max_learning_rate = 0.001\n",
    "\n",
    "for iteration in range(1, 5001):\n",
    "    # Linear warmup\n",
    "    if iteration <= warmup_iterations:\n",
    "        lr = (iteration / warmup_iterations) * max_learning_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    else:\n",
    "        lr = max_learning_rate\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(torch.randn(32, 10))  # Dummy input\n",
    "    loss = ((outputs - torch.randn(32, 1)) ** 2).mean()  # Dummy loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iteration % 100 == 0:\n",
    "        print(f\"Iteration {iteration}, Learning Rate: {lr:.6f}, Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
