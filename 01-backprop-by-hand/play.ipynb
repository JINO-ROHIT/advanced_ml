{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters : 50890\n"
     ]
    }
   ],
   "source": [
    "n_embd = 28 * 28\n",
    "n_hidden = 64\n",
    "n_classes = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "num_epochs = 3\n",
    "\n",
    "g = torch.Generator().manual_seed(2024)\n",
    "\n",
    "#layer 1\n",
    "W1 = torch.randn((n_embd, n_hidden), generator = g) * 0.1\n",
    "b1 = torch.randn(n_hidden, generator = g) * 0.1\n",
    "\n",
    "#layer 2\n",
    "W2 = torch.randn((n_hidden, n_classes), generator = g) * 0.1\n",
    "b2 = torch.randn(n_classes, generator = g) * 0.1\n",
    "\n",
    "parameters = [W1, W2, b1, b2]\n",
    "print(f\"Total Parameters : {sum(p.nelement() for p in parameters)}\")\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 50890\n",
      "Epoch 1, Batch 0/938, Loss: 3.3410\n",
      "Epoch 1, Batch 100/938, Loss: 0.8681\n",
      "Epoch 1, Batch 200/938, Loss: 0.8239\n",
      "Epoch 1, Batch 300/938, Loss: 0.5579\n",
      "Epoch 1, Batch 400/938, Loss: 0.3169\n",
      "Epoch 1, Batch 500/938, Loss: 0.3581\n",
      "Epoch 1, Batch 600/938, Loss: 0.3058\n",
      "Epoch 1, Batch 700/938, Loss: 0.2803\n",
      "Epoch 1, Batch 800/938, Loss: 0.2321\n",
      "Epoch 1, Batch 900/938, Loss: 0.4951\n",
      "Epoch 1, Average Loss: 0.6105\n",
      "Epoch 1, Batch 0/938, Loss: 0.1829\n",
      "Epoch 1, Batch 100/938, Loss: 0.1821\n",
      "Epoch 1, Batch 200/938, Loss: 0.5105\n",
      "Epoch 1, Batch 300/938, Loss: 0.3216\n",
      "Epoch 1, Batch 400/938, Loss: 0.2690\n",
      "Epoch 1, Batch 500/938, Loss: 0.2613\n",
      "Epoch 1, Batch 600/938, Loss: 0.3632\n",
      "Epoch 1, Batch 700/938, Loss: 0.2766\n",
      "Epoch 1, Batch 800/938, Loss: 0.3178\n",
      "Epoch 1, Batch 900/938, Loss: 0.1730\n",
      "Epoch 1, Average Loss: 0.3202\n",
      "Epoch 1, Batch 0/938, Loss: 0.1688\n",
      "Epoch 1, Batch 100/938, Loss: 0.4036\n",
      "Epoch 1, Batch 200/938, Loss: 0.3696\n",
      "Epoch 1, Batch 300/938, Loss: 0.2984\n",
      "Epoch 1, Batch 400/938, Loss: 0.4559\n",
      "Epoch 1, Batch 500/938, Loss: 0.1607\n",
      "Epoch 1, Batch 600/938, Loss: 0.1783\n",
      "Epoch 1, Batch 700/938, Loss: 0.2350\n",
      "Epoch 1, Batch 800/938, Loss: 0.2088\n",
      "Epoch 1, Batch 900/938, Loss: 0.1929\n",
      "Epoch 1, Average Loss: 0.2676\n",
      "Accuracy on training set: 92.67%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2024)\n",
    "\n",
    "# Hyperparameters\n",
    "n_embd = 10\n",
    "n_hidden = 64\n",
    "n_classes = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "num_epochs = 3\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize parameters\n",
    "g = torch.Generator().manual_seed(2024)\n",
    "\n",
    "# Layer 1\n",
    "W1 = torch.randn((28 * 28, n_hidden), generator=g) * 0.1\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1\n",
    "\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, n_classes), generator=g) * 0.1\n",
    "b2 = torch.randn(n_classes, generator=g) * 0.1\n",
    "\n",
    "parameters = [W1, W2, b1, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "print(f\"Total Parameters: {sum(p.nelement() for p in parameters)}\")\n",
    "\n",
    "# Training loop with manual backpropagation\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        data = data.view(data.size(0), -1)  # Flatten the input\n",
    "        h = data @ W1 + b1\n",
    "        h_relu = torch.relu(h)\n",
    "        logits = h_relu @ W2 + b2\n",
    "        loss = F.cross_entropy(logits, target)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass (manual)\n",
    "        # Gradients for output layer\n",
    "        dlogits = F.softmax(logits, dim=1) # we need softmax here since we are using cross_entropy instead of nn.crossentropy that has softmax inbuilt\n",
    "        # using target instead of batch size here since batch size can be uneven\n",
    "        dlogits[range(len(target)), target] -= 1\n",
    "        dlogits /= len(target) \n",
    "\n",
    "        # Gradients for W2 and b2\n",
    "        dW2 = h_relu.t() @ dlogits\n",
    "        db2 = dlogits.sum(0)\n",
    "\n",
    "        # Gradients for h_relu\n",
    "        dh_relu = dlogits @ W2.t()\n",
    "\n",
    "        # Gradients for h (applying ReLU derivative)\n",
    "        dh = dh_relu * (h > 0).float()\n",
    "\n",
    "        # Gradients for W1 and b1\n",
    "        dW1 = data.t() @ dh\n",
    "        db1 = dh.sum(0)\n",
    "\n",
    "        # Update parameters, cannot do an inplace directly\n",
    "        # W1 -= learning_rate * dW1\n",
    "        # b1 -= learning_rate * db1\n",
    "        # W2 -= learning_rate * dW2\n",
    "        # b2 -= learning_rate * db2\n",
    "        grads = [dW1, dW2, db1, db2]\n",
    "        for p, grad in zip(parameters, grads):\n",
    "            p.data += -learning_rate * grad\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch 1, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'Epoch 1, Average Loss: {total_loss / len(train_loader):.4f}')\n",
    "\n",
    "# Evaluation\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in train_loader:\n",
    "        data = data.view(data.size(0), -1)\n",
    "        h = torch.relu(data @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f'Accuracy on training set: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
