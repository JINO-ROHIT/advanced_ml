#### Streaming multiprocessors

A GPU is built around an array of Streaming Multiprocessors (SMs). A multithreaded program is partitioned into blocks of threads that execute independently from each other, so that a GPU with more multiprocessors will automatically execute the program in less time than a GPU with fewer multiprocessors.

Each block will start on one SM and will remain there until it has completed. As soon as it has completed it will retire and another block can be launched on the SM. It's this dynamic scheduling that gives the GPUs the scalability - if you have one SM then all blocks run on the same SM on one big queue, if you have 30 SMs then the blocks will be scheduled across the SMs dynamically. So you should ensure that when you launch a GPU function your grid is composed of a large number of blocks (at least hundreds) to ensure it scales across any GPU.

The warp is the unit of thread scheduling in SMs.
A warp is a set of 32 threads (if you have 128 threads in a block (for example) then threads 0-31 will be in one warp, 32-63 in the next and so on.

if a block has 256 threads, then it has 256 / 32 = 8 warps
so 3 blocks has 24 warps.

24 * 32 = 768 threads, in the G80, this is the maximum number of threads an SM can hold, so the max number of warps in this SM block for G80 is 24 warps.

When an instruction executed by the threads in a warp must wait for the result of a previously initiated long-latency operation, the warp is not selected for execution. Another resident warp that is no longer waiting for results is selected for execution. 

If more than one warp is ready for execution, a priority mechanism is used to select one for execution. This mechanism of filling the latency of expensive operations with work from
other threads is often referred to as `latency hiding`.

Note that warp scheduling is also used for tolerating other types of long latency operations such as pipelined floating-point arithmetic and branch instructions.
With enough warps around, the hardware will likely find a warp to execute at any point in time, thus making full use of the execution hardware in spite of these long-latency operations. The selection of ready warps for execution does not introduce any idle time into the execution timeline, which is referred to as zero-overhead thread scheduling. 
With warp scheduling, the long waiting time of warp instructions is hidden by executing instructions from other warps. This ability to tolerate long
latency operations is the main reason why graphics processing units (GPUs) do not dedicate nearly as much chip area to cache memories as central processing units (CPUs) do. 
As a result, GPUs can dedicate more of their chip area to floating-point execution resources.
