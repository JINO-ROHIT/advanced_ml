{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-28 15:12:45.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mmodel loaded\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model import GPTModel\n",
    "from thop import profile\n",
    "from loguru import logger\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "\n",
    "logger.info(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0e+12 FLOPS for batch size 16\n",
      "5.1e+11 MACS for batch size 16\n",
      "\n",
      "\n",
      "\n",
      "2.0e+12 FLOPS for batch size 32\n",
      "1.0e+12 MACS for batch size 32\n",
      "\n",
      "\n",
      "\n",
      "4.0e+12 FLOPS for batch size 64\n",
      "2.0e+12 MACS for batch size 64\n",
      "\n",
      "\n",
      "\n",
      "8.1e+12 FLOPS for batch size 128\n",
      "4.0e+12 MACS for batch size 128\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MACS = multiply-accumulate operations\n",
    "# MACS are typically counted as two FLOPS (one multiply and one accumulate)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "batch_size = [16, 32, 64, 128]\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for _bs in batch_size:\n",
    "    input_tensor = torch.randint(0, 50257, (_bs, 256)).to(device)\n",
    "    macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "    flops = 2*macs\n",
    "    print(f\"{flops:.1e} FLOPS for batch size {_bs}\")\n",
    "    print(f\"{macs:.1e} MACS for batch size {_bs}\")\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
